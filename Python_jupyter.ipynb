{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1ad68b",
   "metadata": {},
   "source": [
    "# Data is imported\n",
    "### Make Sure that cik_list.xlsx and Python.py are in same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e535f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel ('cik_list.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8f06d",
   "metadata": {},
   "source": [
    "# Complete the url of SECFNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a094974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECFNAME_url']='https://www.sec.gov/Archives/'+df['SECFNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55718378",
   "metadata": {},
   "source": [
    "# Import necessary ibararies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b4cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib,requests, urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c301be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/3662/0000950170-98-000413.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SECFNAME_url'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9fbcbd",
   "metadata": {},
   "source": [
    "# Extract URL text and store it in list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5d0f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "list=[]\n",
    "list_data=[]\n",
    "for i in df['SECFNAME_url']:\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
    "    req= requests.get(i, headers=hdr)\n",
    "    content=req.content.decode()\n",
    "    list.append(req.content.decode())\n",
    "    list_data.append(content)\n",
    "    print(req)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813983f0",
   "metadata": {},
   "source": [
    "# Tokenize text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bbbb085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vipin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sentence_list=[]\n",
    "for i in list_data:\n",
    "    sentence_list.append(nltk.tokenize.sent_tokenize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0687f663",
   "metadata": {},
   "source": [
    "# Remove noise form the text lke non-alapabetical characters and tokenize into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5a2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tokenizedsent_list=[]\n",
    "for i in sentence_list:\n",
    "    word_list=[]\n",
    "    for y in i:\n",
    "        y=y.strip()\n",
    "        processed_text = y.lower()\n",
    "        processed_text = re.sub('[^a-zA-Z]', ' ', processed_text )\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = re.sub('<[^<]+>', '', processed_text)\n",
    "\n",
    "        all_words = nltk.sent_tokenize(processed_text)\n",
    "\n",
    "        word_list+=([nltk.word_tokenize(sent) for sent in all_words])\n",
    "    tokenizedsent_list.append(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b97659",
   "metadata": {},
   "source": [
    "# Import Master dictionary and Stop words\n",
    "### Make sure all the filles are in same folder as Python.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a2bdd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_excel ('LoughranMcDonald_MasterDictionary_2018.xlsx') \n",
    "shortgeneric=open('StopWords_Generic.txt','r').readlines()\n",
    "longgeneric=open('StopWords_GenericLong.txt','r').readlines()\n",
    "currencies=open('StopWords_Currencies.txt','r').readlines()\n",
    "auditor=open('StopWords_Auditor.txt','r').readlines()\n",
    "dataandnumbers=open('StopWords_DatesandNumbers.txt','r').readlines()\n",
    "geographic=open('StopWords_Geographic.txt','r').readlines()\n",
    "names=open('StopWords_Names.txt','r').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b36c87",
   "metadata": {},
   "source": [
    "# Join all the stopwords to form a stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3178221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "for i in shortgeneric,longgeneric,currencies,auditor,dataandnumbers,geographic,names:\n",
    "    for y in i:\n",
    "        y =y.partition('|')[0]\n",
    "        y=y.strip()\n",
    "        y=y.lower()\n",
    "        stopwords.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb76a4",
   "metadata": {},
   "source": [
    "# Creating list of positive,negative and complex wors form Master Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe78142",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive=[]\n",
    "for i in range(len(master['Word'])):\n",
    "    if master['Positive'][i]!=0:\n",
    "        positive.append(master['Word'][i])\n",
    "        \n",
    "negative=[]\n",
    "for i in range(len(master['Word'])):\n",
    "    if master['Negative'][i]!=0:\n",
    "        negative.append(master['Word'][i])\n",
    "\n",
    "complex_words=[]\n",
    "for i in range(len(master['Word'])):\n",
    "    if master['Syllables'][i]>2:\n",
    "        complex_words.append(master['Word'][i].lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a3633",
   "metadata": {},
   "source": [
    "# Importing constraining_dictionary.xlsx and uncertainty_dictionary.xlsx' as lists\n",
    "### Make Sure Both the files are in same folder as Python.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e35cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraining=pd.read_excel ('constraining_dictionary.xlsx')['Word'].to_list()\n",
    "uncertainly=pd.read_excel ('uncertainty_dictionary.xlsx')['Word'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf336c6",
   "metadata": {},
   "source": [
    "# Since positive,negative,constarining and uncertainty are mutually excluxive, a combined dictionary sentiment_dic is made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad6f5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict={}\n",
    "s=-1\n",
    "for i in negative,positive,uncertainly,constraining:\n",
    "    for element in i:\n",
    "        sentiment_dict[element.lower()]=s\n",
    "    s+=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32bee2",
   "metadata": {},
   "source": [
    "# A counter funtion is created that return all the words in a list with their count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e9b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(slist):\n",
    "    dict={}\n",
    "    for y in slist:\n",
    "        if y not in dict.keys():\n",
    "            dict[y]=1\n",
    "        else:\n",
    "            dict[y]+=1\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ab872",
   "metadata": {},
   "source": [
    "# All the word in a document are joined with their count to form a bag of words and stores in dictsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c03150",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictsentence=[]\n",
    "s=0\n",
    "for i in range(len(tokenizedsent_list)):\n",
    "    dummy=[]\n",
    "    for y in range(len(tokenizedsent_list[i])):\n",
    "        dummy+=(tokenizedsent_list[i][y])\n",
    "    dictsentence.append(counter(dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0173f",
   "metadata": {},
   "source": [
    "# Stopwords are removed and the processed dictionaries are stored in procesed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a6209cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesed_sent=[]\n",
    "s=0\n",
    "for i in dictsentence:\n",
    "    dummy=i.copy()\n",
    "    for y in i.keys():     \n",
    "        if y in stopwords:\n",
    "            del dummy[y]\n",
    "    procesed_sent.append(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee7d80",
   "metadata": {},
   "source": [
    "# Each word in the list of dictionary is checked in our sentiment_dict and complex_words list. The count of positive,negative,constraint and uncertaint words for a document are increased accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06ea6d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [921, 2828, 1488, 398, 35144]\n",
      "2 [536, 1396, 1047, 264, 22111]\n",
      "3 [2, 8, 5, 5, 147]\n",
      "4 [341, 1355, 717, 277, 17646]\n",
      "5 [2, 7, 4, 6, 174]\n",
      "6 [42, 387, 92, 143, 3424]\n",
      "7 [116, 565, 272, 180, 5689]\n",
      "8 [59, 347, 106, 167, 3613]\n",
      "9 [2, 5, 3, 4, 164]\n",
      "10 [379, 1518, 502, 459, 18229]\n",
      "11 [2, 5, 3, 5, 167]\n",
      "12 [66, 368, 83, 163, 3397]\n",
      "13 [2, 5, 3, 3, 147]\n",
      "14 [129, 515, 151, 243, 5250]\n",
      "15 [247, 1201, 336, 550, 13433]\n",
      "16 [134, 641, 169, 270, 5676]\n",
      "17 [101, 625, 157, 248, 5176]\n",
      "18 [3, 3, 3, 2, 133]\n",
      "19 [144, 855, 195, 336, 7381]\n",
      "20 [297, 1329, 523, 320, 15839]\n",
      "21 [28, 62, 15, 8, 1988]\n",
      "22 [160, 1120, 652, 163, 11506]\n",
      "23 [58, 227, 74, 127, 4689]\n",
      "24 [29, 138, 63, 74, 2775]\n",
      "25 [55, 120, 67, 118, 3348]\n",
      "26 [99, 253, 112, 174, 4762]\n",
      "27 [181, 889, 374, 110, 12621]\n",
      "28 [254, 740, 271, 383, 12676]\n",
      "29 [350, 1763, 1305, 207, 21585]\n",
      "30 [64, 129, 66, 102, 3483]\n",
      "31 [164, 404, 167, 126, 7188]\n",
      "32 [273, 702, 300, 440, 12719]\n",
      "33 [102, 204, 94, 34, 4738]\n",
      "34 [49, 185, 85, 74, 2750]\n",
      "35 [176, 463, 277, 263, 8837]\n",
      "36 [25, 54, 34, 14, 782]\n",
      "37 [31, 120, 41, 49, 1909]\n",
      "38 [38, 157, 41, 54, 2107]\n",
      "39 [59, 269, 114, 80, 3502]\n",
      "40 [368, 1212, 1075, 367, 20897]\n",
      "41 [33, 110, 37, 45, 1966]\n",
      "42 [41, 165, 44, 55, 2255]\n",
      "43 [36, 121, 42, 50, 2121]\n",
      "44 [26, 101, 32, 40, 2028]\n",
      "45 [27, 101, 35, 39, 1918]\n",
      "46 [459, 1122, 464, 178, 18937]\n",
      "47 [19, 65, 24, 24, 1386]\n",
      "48 [56, 239, 80, 31, 3778]\n",
      "49 [47, 231, 55, 34, 3092]\n",
      "50 [112, 404, 121, 146, 6384]\n",
      "51 [2, 10, 5, 2, 165]\n",
      "52 [15, 46, 27, 34, 1238]\n",
      "53 [19, 136, 19, 31, 1552]\n",
      "54 [116, 825, 274, 179, 13100]\n",
      "55 [663, 2746, 794, 391, 51695]\n",
      "56 [78, 404, 105, 116, 5864]\n",
      "57 [218, 999, 379, 212, 15758]\n",
      "58 [136, 614, 174, 168, 9278]\n",
      "59 [5, 83, 30, 12, 1398]\n",
      "60 [315, 975, 364, 341, 22412]\n",
      "61 [86, 390, 94, 99, 6329]\n",
      "62 [184, 4405, 283, 295, 13475]\n",
      "63 [174, 4109, 248, 223, 10890]\n",
      "64 [635, 3040, 678, 592, 72222]\n",
      "65 [793, 3142, 650, 626, 80464]\n",
      "66 [24, 81, 26, 40, 1755]\n",
      "67 [311, 813, 360, 251, 11555]\n",
      "68 [21, 22, 17, 48, 1306]\n",
      "69 [64, 145, 58, 67, 3028]\n",
      "70 [19, 26, 21, 50, 1366]\n",
      "71 [179, 288, 143, 206, 7633]\n",
      "72 [21, 28, 20, 37, 1145]\n",
      "73 [70, 172, 79, 59, 3161]\n",
      "74 [67, 189, 87, 71, 2933]\n",
      "75 [2, 2, 5, 2, 116]\n",
      "76 [234, 438, 186, 242, 9240]\n",
      "77 [255, 438, 182, 255, 9242]\n",
      "78 [564, 659, 514, 368, 19110]\n",
      "79 [110, 165, 64, 96, 3826]\n",
      "80 [185, 236, 97, 158, 6393]\n",
      "81 [200, 304, 113, 218, 7268]\n",
      "82 [248, 406, 113, 199, 6862]\n",
      "83 [1126, 2074, 1043, 892, 40952]\n",
      "84 [1126, 2085, 1045, 895, 41197]\n",
      "85 [201, 436, 173, 243, 8336]\n",
      "86 [202, 444, 174, 241, 8400]\n",
      "87 [142, 352, 101, 177, 5860]\n",
      "88 [146, 434, 87, 181, 4968]\n",
      "89 [14, 25, 17, 13, 896]\n",
      "90 [15, 31, 21, 17, 1067]\n",
      "91 [175, 384, 107, 155, 6322]\n",
      "92 [20, 67, 27, 23, 1206]\n",
      "93 [50, 175, 68, 34, 2877]\n",
      "94 [33, 82, 32, 30, 1730]\n",
      "95 [170, 315, 98, 154, 5309]\n",
      "96 [18, 62, 26, 32, 1114]\n",
      "97 [28, 187, 43, 66, 2016]\n",
      "98 [30, 166, 43, 72, 1990]\n",
      "99 [312, 978, 409, 295, 14461]\n",
      "100 [17, 127, 37, 31, 1119]\n",
      "101 [27, 140, 36, 48, 1398]\n",
      "102 [28, 161, 36, 53, 1428]\n",
      "103 [649, 2896, 1498, 603, 32519]\n",
      "104 [26, 115, 44, 55, 1324]\n",
      "105 [27, 138, 45, 92, 1734]\n",
      "106 [33, 211, 77, 131, 2437]\n",
      "107 [181, 468, 233, 312, 7229]\n",
      "108 [17, 112, 40, 51, 1219]\n",
      "109 [76, 358, 117, 82, 5021]\n",
      "110 [63, 260, 89, 102, 2867]\n",
      "111 [24, 132, 44, 51, 1594]\n",
      "112 [29, 180, 56, 61, 2121]\n",
      "113 [2, 14, 3, 3, 168]\n",
      "114 [724, 2074, 802, 513, 25929]\n",
      "115 [63, 287, 114, 92, 3160]\n",
      "116 [106, 354, 162, 147, 4029]\n",
      "117 [132, 850, 284, 194, 15809]\n",
      "118 [821, 3104, 1052, 436, 64213]\n",
      "119 [80, 418, 114, 132, 6641]\n",
      "120 [224, 1017, 391, 236, 17746]\n",
      "121 [144, 637, 183, 195, 11158]\n",
      "122 [5, 82, 30, 12, 1404]\n",
      "123 [357, 1818, 443, 400, 38157]\n",
      "124 [87, 400, 98, 103, 7025]\n",
      "125 [189, 4605, 307, 315, 14035]\n",
      "126 [183, 4360, 274, 246, 11515]\n",
      "127 [9, 12, 7, 11, 676]\n",
      "128 [10, 12, 7, 13, 644]\n",
      "129 [167, 427, 169, 128, 7041]\n",
      "130 [31, 164, 57, 19, 2036]\n",
      "131 [31, 62, 27, 23, 1064]\n",
      "132 [32, 108, 41, 34, 1433]\n",
      "133 [170, 339, 138, 143, 6513]\n",
      "134 [17, 65, 27, 39, 1190]\n",
      "135 [55, 227, 75, 66, 3266]\n",
      "136 [33, 152, 30, 50, 1692]\n",
      "137 [79, 239, 37, 35, 2370]\n",
      "138 [384, 997, 392, 126, 15783]\n",
      "139 [14, 49, 9, 18, 726]\n",
      "140 [22, 16, 14, 17, 992]\n",
      "141 [21, 70, 12, 28, 830]\n",
      "142 [59, 252, 87, 42, 3522]\n",
      "143 [655, 1658, 1243, 221, 26512]\n",
      "144 [15, 31, 10, 5, 502]\n",
      "145 [20, 16, 16, 20, 930]\n",
      "146 [32, 32, 10, 14, 629]\n",
      "147 [25, 68, 12, 15, 668]\n",
      "148 [176, 4484, 146, 135, 6069]\n",
      "149 [2, 80, 3, 3, 135]\n",
      "150 [23, 983, 25, 51, 1307]\n",
      "151 [177, 4487, 145, 135, 6038]\n",
      "152 [32, 1721, 39, 61, 1720]\n"
     ]
    }
   ],
   "source": [
    "score_sentiment=[]\n",
    "s=0\n",
    "for i in procesed_sent:\n",
    "    positive_count=0\n",
    "    negative_count=0\n",
    "    complex_score=0\n",
    "    constraining_count=0\n",
    "    uncertainty_count=0\n",
    "    for y in i.keys():\n",
    "        if y in sentiment_dict.keys():\n",
    "            if(sentiment_dict[y]==-1):\n",
    "                negative_count+=i[y]\n",
    "            elif(sentiment_dict[y]==1):\n",
    "                positive_count+=i[y]\n",
    "            elif(sentiment_dict[y]==3):\n",
    "                uncertainty_count+=i[y]\n",
    "            elif(sentiment_dict[y]==5):\n",
    "                constraining_count+=i[y]\n",
    "        if y in complex_words:\n",
    "            complex_score+=i[y]  \n",
    "    s+=1\n",
    "    score_sentiment.append([positive_count,negative_count,constraining_count,uncertainty_count,complex_score])\n",
    "    print(s,[positive_count,negative_count,constraining_count,uncertainty_count,complex_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f73297",
   "metadata": {},
   "source": [
    "# The total count of words and sentecnces for each document are calculated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c15c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=[]\n",
    "for i in range(len(procesed_sent)):\n",
    "    count=0\n",
    "    for y in procesed_sent[i].keys():\n",
    "        count+=procesed_sent[i][y]\n",
    "    total_data.append([count,len(tokenizedsent_list[i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713a43e",
   "metadata": {},
   "source": [
    "# The list of respective output columns are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bec1709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "positive_score=[]\n",
    "negative_score=[]\n",
    "polarity_score=[]\n",
    "average_sentence_length=[]\n",
    "percentage_of_complex_words=[]\n",
    "fog_index=[]\n",
    "complex_word_count=[]\n",
    "word_count=[]\n",
    "uncertainty_score=[]\n",
    "constraining_score=[]\n",
    "positive_word_proportion=[]\n",
    "negative_word_proportion=[]\n",
    "uncertainty_word_proportion=[]\n",
    "constraining_word_proportion=[]\n",
    "constraining_words_whole_report=[]\n",
    "count_cwwr=0\n",
    "for cik in range(len(procesed_sent)):\n",
    "    cik_p=score_sentiment[cik][0]\n",
    "    cik_n=score_sentiment[cik][1]\n",
    "    cik_c=score_sentiment[cik][2]\n",
    "    cik_u=score_sentiment[cik][3]\n",
    "    cik_cwc=score_sentiment[cik][4]\n",
    "    cik_tw=total_data[cik][0]\n",
    "    cik_ts=total_data[cik][1]\n",
    "    positive_score.append(cik_p)\n",
    "    negative_score.append(cik_n)\n",
    "    constraining_score.append(cik_c)\n",
    "    count_cwwr+=cik_c\n",
    "    uncertainty_score.append(cik_u)\n",
    "    complex_word_count.append(cik_cwc)\n",
    "    word_count.append(cik_tw)\n",
    "    average_sentence_length.append(cik_tw/cik_ts)\n",
    "    percentage_of_complex_words.append(cik_cwc/cik_tw)\n",
    "    fog_index.append(0.4*(cik_tw/cik_ts+cik_cwc/cik_tw))\n",
    "    positive_word_proportion.append(cik_p/cik_tw)\n",
    "    negative_word_proportion.append(cik_n/cik_tw)\n",
    "    constraining_word_proportion.append(cik_c/cik_tw)\n",
    "    uncertainty_word_proportion.append(cik_u/cik_tw)\n",
    "    polarity_score.append((cik_p-cik_n)/(cik_p+cik_n+0.000001))\n",
    "    if(cik==(len(procesed_sent)-1)):\n",
    "        constraining_words_whole_report.insert(0,count_cwwr)\n",
    "    else:\n",
    "        constraining_words_whole_report.append('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae133e",
   "metadata": {},
   "source": [
    "# The list are exported as Output_Score.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b25d975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {'CIK':df['CIK'].to_list(),\n",
    "        'CONAME':df['CONAME'].to_list(),\n",
    "        'FYRMO':df['FYRMO'].to_list(),\n",
    "        'FDATE':df['FDATE'].to_list(),\n",
    "        'FORM':df['FORM'].to_list(),\n",
    "        'SECFNAME':df['SECFNAME'].to_list(),\n",
    "        'positive_score':positive_score,\n",
    "        'negative_score':negative_score,\n",
    "        'polarity_score':polarity_score,\n",
    "        'average_sentence_length':average_sentence_length,\n",
    "        'percentage_of_complex_words':percentage_of_complex_words,\n",
    "        'fog_index':fog_index,\n",
    "        'complex_word_count':complex_word_count,\n",
    "        'word_count':word_count,\n",
    "        'uncertainty_score':uncertainty_score,\n",
    "        'constraining_score':constraining_score,\n",
    "        'positive_word_proportion':positive_word_proportion,\n",
    "        'negative_word_proportion':negative_word_proportion,\n",
    "        'uncertainty_word_proportion':uncertainty_word_proportion,\n",
    "        'constraining_word_proportion':constraining_word_proportion,\n",
    "        'constraining_words_whole_report':constraining_words_whole_report}\n",
    "\n",
    "\n",
    "\n",
    "file= pd.DataFrame(data, columns = ['CIK','CONAME','FYRMO','FDATE','FORM','SECFNAME','positive_score','negative_score',\n",
    "                                    'polarity_score','average_sentence_length','percentage_of_complex_words','fog_index','complex_word_count',\n",
    "                                   'word_count','uncertainty_score','constraining_score','positive_word_proportion','negative_word_proportion',\n",
    "                                   'uncertainty_word_proportion','constraining_word_proportion','constraining_words_whole_report'])\n",
    "\n",
    "file.to_excel ('Output_Score.xlsx', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a7f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d302dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b7309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
